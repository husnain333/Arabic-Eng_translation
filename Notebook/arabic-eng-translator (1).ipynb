{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"name":"Transformer-NMT-en-es.ipynb","provenance":[{"file_id":"16JmM4d_Wc3Eh1-iyjkA5_kw8qLDDFhr2","timestamp":1602519669782},{"file_id":"1i4a26jVRVsAIfAGCe5pnl4l51B6GvjWs","timestamp":1595454386969},{"file_id":"17dOcF-VlAVBY0vzgqaANGQ4taSNc2Wp6","timestamp":1571411676663}],"collapsed_sections":[],"toc_visible":true,"machine_shape":"hm"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Loading the libraries","metadata":{"id":"fN7pJARIt9uK"}},{"cell_type":"code","source":"import math\nimport os\nimport gc\nimport time\nimport re\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n \n\nfrom google.colab import drive","metadata":{"id":"ZbcvtPlp3YWu","executionInfo":{"status":"ok","timestamp":1604164306003,"user_tz":-60,"elapsed":1499,"user":{"displayName":"Eduardo Mu√±oz Sala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9gTFB7xfdxQyNiKRUzqE5vwb9WQjQuMtQz5F16w=s64","userId":"13317831924226771761"}},"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T13:39:08.392769Z","iopub.execute_input":"2025-03-03T13:39:08.393047Z","iopub.status.idle":"2025-03-03T13:39:08.724848Z","shell.execute_reply.started":"2025-03-03T13:39:08.393025Z","shell.execute_reply":"2025-03-03T13:39:08.724213Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import tensorflow as tf\n\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import backend as K\nimport tensorflow_datasets as tfds\n\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences","metadata":{"id":"P6o_cpZz3y_-","executionInfo":{"status":"ok","timestamp":1604164318361,"user_tz":-60,"elapsed":2911,"user":{"displayName":"Eduardo Mu√±oz Sala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9gTFB7xfdxQyNiKRUzqE5vwb9WQjQuMtQz5F16w=s64","userId":"13317831924226771761"}},"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T13:39:08.725536Z","iopub.execute_input":"2025-03-03T13:39:08.725820Z","iopub.status.idle":"2025-03-03T13:39:22.876518Z","shell.execute_reply.started":"2025-03-03T13:39:08.725800Z","shell.execute_reply":"2025-03-03T13:39:22.875857Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"from datasets import load_dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T13:39:22.878093Z","iopub.execute_input":"2025-03-03T13:39:22.878610Z","iopub.status.idle":"2025-03-03T13:39:24.367572Z","shell.execute_reply.started":"2025-03-03T13:39:22.878585Z","shell.execute_reply":"2025-03-03T13:39:24.366923Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"Setting some parameters and hyperparameters for our model","metadata":{"id":"bqyCyv2b5iif"}},{"cell_type":"code","source":"# Parameters for our model\nINPUT_COLUMN = 'input'\nTARGET_COLUMN = 'target'\nNUM_SAMPLES = 80000 #40000\nMAX_VOCAB_SIZE = 2**14\n\nBATCH_SIZE = 64  # Batch size for training.\nEPOCHS = 15  # Number of epochs to train for.\nMAX_LENGTH = 15","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T13:39:24.368618Z","iopub.execute_input":"2025-03-03T13:39:24.369070Z","iopub.status.idle":"2025-03-03T13:39:24.372704Z","shell.execute_reply.started":"2025-03-03T13:39:24.369047Z","shell.execute_reply":"2025-03-03T13:39:24.371915Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"\n\n# Global parameters\nroot_folder='/content/drive'\ndata_folder_name='My Drive/datasets/ara_eng_translations'\ncheckpoint_folder = \"My Drive/Projects/Transformer_NMT/ckpt/\"\ndataset = load_dataset('Helsinki-NLP/tatoeba_mt', 'ara-eng', trust_remote_code=True)\n# Variable for data directory\nDATA_PATH = os.path.abspath(os.path.join(root_folder, data_folder_name))\ncheckpoint_path = os.path.abspath(os.path.join(root_folder, checkpoint_folder))\n\n# Both train and test set are in the root data directory\ntrain_path = DATA_PATH\n","metadata":{"id":"rTr3QS6_5tBO","executionInfo":{"status":"ok","timestamp":1604164327404,"user_tz":-60,"elapsed":984,"user":{"displayName":"Eduardo Mu√±oz Sala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9gTFB7xfdxQyNiKRUzqE5vwb9WQjQuMtQz5F16w=s64","userId":"13317831924226771761"}},"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T13:39:24.373538Z","iopub.execute_input":"2025-03-03T13:39:24.373797Z","iopub.status.idle":"2025-03-03T13:39:29.949840Z","shell.execute_reply.started":"2025-03-03T13:39:24.373770Z","shell.execute_reply":"2025-03-03T13:39:29.949191Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/12.1k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0a5095d6f7f64b6fbc2e3bc6011d15ee"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tatoeba_mt.py:   0%|          | 0.00/15.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0cd67cbdde3047759c6ca5119e82ad94"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"dataset_infos.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"865424b2cdcf4818ab81dbe9eed4fa3a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tatoeba-test.ara-eng.tsv:   0%|          | 0.00/938k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4cb1ec513c9847e987cfc4235af25bce"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tatoeba-dev.ara-eng.tsv:   0%|          | 0.00/1.78M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d79559dfed5f4ad995f589eea792247f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/10304 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e172aead191f4f86b36fe4a366dfbb59"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/19528 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"beededeaa7dc434386dd6790209b6771"}},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"def preprocess_text_nonbreaking(corpus, non_breaking_prefixes):\n  corpus_cleaned = corpus\n  # Add the string $$$ before the non breaking prefixes\n  # To avoid remove dots from some words\n  for prefix in non_breaking_prefixes:\n    corpus_cleaned = corpus_cleaned.replace(prefix, prefix + '$$$')\n  # Remove dots not at the end of a sentence\n  corpus_cleaned = re.sub(r\"\\.(?=[0-9]|[a-z]|[A-Z])\", \".$$$\", corpus_cleaned)\n  # Remove the $$$ mark\n  corpus_cleaned = re.sub(r\"\\.\\$\\$\\$\", '', corpus_cleaned)\n  # Rmove multiple white spaces\n  corpus_cleaned = re.sub(r\"  +\", \" \", corpus_cleaned)\n\n  return corpus_cleaned\n","metadata":{"id":"xnomPENWc3gD","executionInfo":{"status":"ok","timestamp":1604164333039,"user_tz":-60,"elapsed":907,"user":{"displayName":"Eduardo Mu√±oz Sala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9gTFB7xfdxQyNiKRUzqE5vwb9WQjQuMtQz5F16w=s64","userId":"13317831924226771761"}},"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T13:39:29.950660Z","iopub.execute_input":"2025-03-03T13:39:29.950992Z","iopub.status.idle":"2025-03-03T13:39:29.955196Z","shell.execute_reply.started":"2025-03-03T13:39:29.950960Z","shell.execute_reply":"2025-03-03T13:39:29.954313Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"## Loading the dataset","metadata":{"id":"bPlOT-2mlw0r"}},{"cell_type":"code","source":"print(dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T13:39:29.956049Z","iopub.execute_input":"2025-03-03T13:39:29.956263Z","iopub.status.idle":"2025-03-03T13:39:29.986918Z","shell.execute_reply.started":"2025-03-03T13:39:29.956234Z","shell.execute_reply":"2025-03-03T13:39:29.986188Z"}},"outputs":[{"name":"stdout","text":"DatasetDict({\n    test: Dataset({\n        features: ['sourceLang', 'targetlang', 'sourceString', 'targetString'],\n        num_rows: 10304\n    })\n    validation: Dataset({\n        features: ['sourceLang', 'targetlang', 'sourceString', 'targetString'],\n        num_rows: 19528\n    })\n})\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"df = pd.DataFrame({\n    INPUT_COLUMN: dataset['validation']['sourceString'],  # English sentences\n    TARGET_COLUMN: dataset['validation']['targetString']  # Arabic sentences\n})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T13:39:29.987603Z","iopub.execute_input":"2025-03-03T13:39:29.987790Z","iopub.status.idle":"2025-03-03T13:39:30.046225Z","shell.execute_reply.started":"2025-03-03T13:39:29.987774Z","shell.execute_reply":"2025-03-03T13:39:30.045641Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"df = df.iloc[:NUM_SAMPLES]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T13:39:30.046955Z","iopub.execute_input":"2025-03-03T13:39:30.047136Z","iopub.status.idle":"2025-03-03T13:39:30.050458Z","shell.execute_reply.started":"2025-03-03T13:39:30.047121Z","shell.execute_reply":"2025-03-03T13:39:30.049779Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"input_data = df[INPUT_COLUMN].tolist()  \ntarget_data = df[TARGET_COLUMN].tolist() ","metadata":{"id":"fjKW8DnObtvl","executionInfo":{"status":"ok","timestamp":1604164376760,"user_tz":-60,"elapsed":2367,"user":{"displayName":"Eduardo Mu√±oz Sala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9gTFB7xfdxQyNiKRUzqE5vwb9WQjQuMtQz5F16w=s64","userId":"13317831924226771761"}},"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T13:39:30.051362Z","iopub.execute_input":"2025-03-03T13:39:30.051673Z","iopub.status.idle":"2025-03-03T13:39:30.069438Z","shell.execute_reply.started":"2025-03-03T13:39:30.051630Z","shell.execute_reply":"2025-03-03T13:39:30.068715Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# Print sample sentences\nprint('Number of sentences:', len(input_data))\nprint(input_data[:5])  # Sample English sentences\nprint(target_data[:5])  # Sample Arabic translations","metadata":{"id":"0G8Wa9FBdy5Y","executionInfo":{"status":"ok","timestamp":1604164410474,"user_tz":-60,"elapsed":3604,"user":{"displayName":"Eduardo Mu√±oz Sala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9gTFB7xfdxQyNiKRUzqE5vwb9WQjQuMtQz5F16w=s64","userId":"13317831924226771761"}},"outputId":"b33db2f5-5781-4eb6-bb72-5af73d1d823e","colab":{"base_uri":"https://localhost:8080/"},"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T13:39:30.072369Z","iopub.execute_input":"2025-03-03T13:39:30.072593Z","iopub.status.idle":"2025-03-03T13:39:30.087148Z","shell.execute_reply.started":"2025-03-03T13:39:30.072574Z","shell.execute_reply":"2025-03-03T13:39:30.086407Z"}},"outputs":[{"name":"stdout","text":"Number of sentences: 19528\n['ÿπŸÖÿ±ŸÉ ÿ±ÿßŸäÿ≠ ÿßŸÑŸÖŸÉÿ≥ŸäŸÉÿü', 'ŸÅŸÉÿ±ŸÜÿß ÿßŸÜŸá ÿ∑ÿ®ŸäÿπŸä ŸÑÿßÿ≤ŸÖ Ÿäÿ™ÿπÿßŸÇÿ®.', 'ŸÑÿßÿ≤ŸÖ ÿ™ÿ™ÿ±ŸÉ ÿßŸÑÿßŸÖŸàÿ± ÿ™ÿßÿÆÿ∞ ŸÖÿ¨ÿ±ÿßŸáÿß ÿßŸÑÿ∑ÿ®ŸäÿπŸä.', 'ŸÑÿß Ÿäÿ±ŸäÿØŸàŸÜ ÿßÿ≥ÿ™ÿÆ.', 'ŸÖŸÇÿ®ŸÑÿ™ ÿ™ÿ≠ÿ¨Ÿä ÿ¥Ÿä ÿßŸÉÿ´ÿ± ÿπŸÜ ÿßŸÑŸÖŸàÿ∂Ÿàÿπ.']\n['Have you ever been to Mexico?', 'We thought that it was natural that he should be punished.', 'You must let things take their own course.', \"They don't want to use it.\", 'She declined to say more about it.']\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"#Delete the dataframe and release the memory (if it is possible)\ndel df\ngc.collect()","metadata":{"id":"TMAFFdpIyNZd","executionInfo":{"status":"ok","timestamp":1604164414243,"user_tz":-60,"elapsed":1072,"user":{"displayName":"Eduardo Mu√±oz Sala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9gTFB7xfdxQyNiKRUzqE5vwb9WQjQuMtQz5F16w=s64","userId":"13317831924226771761"}},"outputId":"db773f7b-1723-47a4-f7a5-3f3b4572a8a6","colab":{"base_uri":"https://localhost:8080/"},"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T13:39:30.088353Z","iopub.execute_input":"2025-03-03T13:39:30.088562Z","iopub.status.idle":"2025-03-03T13:39:30.289888Z","shell.execute_reply.started":"2025-03-03T13:39:30.088542Z","shell.execute_reply":"2025-03-03T13:39:30.288965Z"}},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"89"},"metadata":{}}],"execution_count":12},{"cell_type":"markdown","source":"# Tokenize the text data\n\nNext, let's see how to prepare the data for our model. It is very simple and the steps are the following:\n\n- Create the vocabulary from the corpus using Subword tokenization, breaking words into ‚Äúsubword units‚Äù - strings of characters like ing or eau - that allow the downstream model to make intelligent decisions on words it doesn‚Äôt recognize.\n- Calculate the maximum length of the input and output sequences.\n- Tokenize the data, convert the raw text into a sequence of integers. Once we define the vocabulary, we use the encode method to get the token for every word in the corpus.\n- Remove sentences longer that the max length defined.\n- Padding the sentences: we need to pad zeros at the end of the sequences so that all sequences have the same length. Otherwise, we won't be able train the model on batches","metadata":{"id":"TEFw0D2vP_Dl"}},{"cell_type":"code","source":"def subword_tokenize(corpus, vocab_size, max_length):\n  # Create the vocabulary using Subword tokenization\n  tokenizer_corpus = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n    corpus, target_vocab_size=vocab_size)\n  # Get the final vocab size, adding the eos and sos tokens\n  num_words = tokenizer_corpus.vocab_size + 2\n  # Set eos and sos token\n  sos_token = [num_words-2]\n  eos_token = [num_words-1]\n  # Tokenize the corpus\n  sentences = [sos_token + tokenizer_corpus.encode(sentence) + eos_token\n          for sentence in corpus]\n  # Identify the index of the sentences longer than max length\n  idx_to_remove = [count for count, sent in enumerate(sentences)\n                 if len(sent) > max_length]\n  #Pad the sentences\n  sentences = tf.keras.preprocessing.sequence.pad_sequences(sentences,\n                                                       value=0,\n                                                       padding='post',\n                                                       maxlen=max_length)\n  \n  return sentences, tokenizer_corpus, num_words, sos_token, eos_token, idx_to_remove\n","metadata":{"id":"khsaxokjofHr","executionInfo":{"status":"ok","timestamp":1604164418564,"user_tz":-60,"elapsed":909,"user":{"displayName":"Eduardo Mu√±oz Sala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9gTFB7xfdxQyNiKRUzqE5vwb9WQjQuMtQz5F16w=s64","userId":"13317831924226771761"}},"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T13:39:30.290859Z","iopub.execute_input":"2025-03-03T13:39:30.291176Z","iopub.status.idle":"2025-03-03T13:39:30.306319Z","shell.execute_reply.started":"2025-03-03T13:39:30.291144Z","shell.execute_reply":"2025-03-03T13:39:30.305567Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# Tokenize and pad the input sequences\nencoder_inputs, tokenizer_inputs, num_words_inputs, sos_token_input, eos_token_input, del_idx_inputs= subword_tokenize(input_data, \n                                                                                                        MAX_VOCAB_SIZE, MAX_LENGTH)\n# Tokenize and pad the outputs sequences\ndecoder_outputs, tokenizer_outputs, num_words_output, sos_token_output, eos_token_output, del_idx_outputs = subword_tokenize(target_data, \n                                                                                                        MAX_VOCAB_SIZE, MAX_LENGTH)","metadata":{"id":"onrwROJrquij","executionInfo":{"status":"ok","timestamp":1604164475693,"user_tz":-60,"elapsed":54005,"user":{"displayName":"Eduardo Mu√±oz Sala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9gTFB7xfdxQyNiKRUzqE5vwb9WQjQuMtQz5F16w=s64","userId":"13317831924226771761"}},"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T13:39:30.307077Z","iopub.execute_input":"2025-03-03T13:39:30.307270Z","iopub.status.idle":"2025-03-03T13:39:55.993412Z","shell.execute_reply.started":"2025-03-03T13:39:30.307253Z","shell.execute_reply":"2025-03-03T13:39:55.992735Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"# Check the tokenize function\nprint(encoder_inputs[:5], sos_token_input, eos_token_input)\nprint(decoder_outputs[:5], sos_token_output, eos_token_output)","metadata":{"id":"gYtibBaqsAxA","executionInfo":{"status":"ok","timestamp":1604164485208,"user_tz":-60,"elapsed":1000,"user":{"displayName":"Eduardo Mu√±oz Sala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9gTFB7xfdxQyNiKRUzqE5vwb9WQjQuMtQz5F16w=s64","userId":"13317831924226771761"}},"outputId":"abff64f4-e25b-4531-f148-cf7c39cfec94","colab":{"base_uri":"https://localhost:8080/"},"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T13:39:55.994221Z","iopub.execute_input":"2025-03-03T13:39:55.994439Z","iopub.status.idle":"2025-03-03T13:39:56.002610Z","shell.execute_reply.started":"2025-03-03T13:39:55.994420Z","shell.execute_reply":"2025-03-03T13:39:56.001708Z"}},"outputs":[{"name":"stdout","text":"[[11136  4556 10912  1964  6615     4 11137     0     0     0     0     0\n      0     0     0]\n [11136  8356    82  1038  8570   721  5420   572 10926 11137     0     0\n      0     0     0]\n [11136   721  6440  5134  1069   743   737  7961  6709 10926 11137     0\n      0     0     0]\n [11136     7  1860   862   200 10926 11137     0     0     0     0     0\n      0     0     0]\n [11136  7838   508  4876  1762  1944 10376    42  2330 10926 11137     0\n      0     0     0]] [11136] [11137]\n[[11223   236     7   317    90     2  3103 11030 11224     0     0     0\n      0     0     0]\n [11223    41   279    18    44    13  2418    18    43   111    35  6910\n  11013 11224     0]\n [11223    27   126   373   322   157   163   330  1084 11013 11224     0\n      0     0     0]\n [11223    88    36 11006     9    50     2   321    63 11013 11224     0\n      0     0     0]\n [11223    38  5092     2   189   124    69    63 11013 11224     0     0\n      0     0     0]] [11223] [11224]\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"print('Size of Input Vocabulary: ', num_words_inputs)\nprint('Size of Output Vocabulary: ', num_words_output)","metadata":{"id":"VF4l2stYEu2l","executionInfo":{"status":"ok","timestamp":1604164491249,"user_tz":-60,"elapsed":987,"user":{"displayName":"Eduardo Mu√±oz Sala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9gTFB7xfdxQyNiKRUzqE5vwb9WQjQuMtQz5F16w=s64","userId":"13317831924226771761"}},"outputId":"52b708b7-415b-4a46-a1f4-83673abc2938","colab":{"base_uri":"https://localhost:8080/"},"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T13:39:56.003642Z","iopub.execute_input":"2025-03-03T13:39:56.004000Z","iopub.status.idle":"2025-03-03T13:39:56.032209Z","shell.execute_reply.started":"2025-03-03T13:39:56.003968Z","shell.execute_reply":"2025-03-03T13:39:56.031537Z"}},"outputs":[{"name":"stdout","text":"Size of Input Vocabulary:  11138\nSize of Output Vocabulary:  11225\n","output_type":"stream"}],"execution_count":16},{"cell_type":"markdown","source":"# Create the batch data generator","metadata":{"id":"Ypm8h5aZQTZ1"}},{"cell_type":"markdown","source":"- Create a batch data generator: we want to train the model on batches, group of sentences, so we need to create a Dataset using the tf.data library and the function batch_on_slices on the input and output sequences.","metadata":{"id":"9FP0WPsdM8hl"}},{"cell_type":"code","source":"# Define a dataset \ndataset = tf.data.Dataset.from_tensor_slices(\n    (encoder_inputs, decoder_outputs))\ndataset = dataset.shuffle(len(input_data), reshuffle_each_iteration=True).batch(\n    BATCH_SIZE, drop_remainder=True)\n\ndataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)","metadata":{"id":"wFxMp3TOIYff","executionInfo":{"status":"ok","timestamp":1604164511134,"user_tz":-60,"elapsed":6333,"user":{"displayName":"Eduardo Mu√±oz Sala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9gTFB7xfdxQyNiKRUzqE5vwb9WQjQuMtQz5F16w=s64","userId":"13317831924226771761"}},"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T13:39:56.032865Z","iopub.execute_input":"2025-03-03T13:39:56.033075Z","iopub.status.idle":"2025-03-03T13:39:57.178048Z","shell.execute_reply.started":"2025-03-03T13:39:56.033047Z","shell.execute_reply":"2025-03-03T13:39:57.177088Z"}},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":"# Building a Transformer","metadata":{"id":"ycT0YqydRcUd"}},{"cell_type":"markdown","source":"## What is Transformer?\n*\"In this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output. The Transformer allows for significantly more parallelization¬†‚Ä¶ the Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequence-aligned RNNs or convolution.\"*\n\n\"Attention is all you need\" paper\n\nThe Transformer model extract  features for each word using a self-attention mechanism to figure out how important all the other words in the sentence are w.r.t. to the aforementioned word. And no recurrent units are used to obtain this features, they are just weighted sum and activations, so they can be very parallelizable and efficient.\n\nBut We will dive deeper to understand what all this means.","metadata":{"id":"Y-iKViHeWz20"}},{"cell_type":"markdown","source":"# Self-attention: the fundamental operation\n\n*\"Self-attention is a sequence-to-sequence operation: a sequence of vectors goes in, and a sequence of vectors comes out. Let's call the input vectors ùê±1,ùê±2,‚Ä¶ùê±t and the corresponding output vectors ùê≤1,ùê≤2,‚Ä¶,ùê≤t. The vectors all have dimension k. To produce output vector ùê≤i, the self attention operation simply takes a weighted average over all the input vectors, the simplest option is the dot product.\"*\n\n**Transformers from scratch by Peter Bloem**\n\nIn the self-attention mechanism of our model we need to introduce three elements: Queries, Values and Keys\n\n","metadata":{"id":"lcw8YIQqRhOJ"}},{"cell_type":"markdown","source":"## Queries, Keys and Values\nEvery input vector is used in three different ways in the self-attention mechanism: the Query, the Key and the Value. In every role, it is compared to the others vectors to get its own output yi (Query), to get the j-th output yj (Key) and to compute each output vector once the weights have been established (Value).\n\nTo obtain this roles, we need three weight matrices of dimensions k x k and compute three linear transformation for each xi:\n\n![Alt](images/query_key_value.png \"title \"Transformers from scratch\" by Peter Bloem\")\n\nThese three matrices are usually known as K, Q and V, three learnable weight layers that are applied to the same encoded input. Consequently, as each of these three matrices come from the same input, we can apply the attention mechanism of the input vector with itself, a \"self-attention\".\n","metadata":{"id":"UBlUGBq9eFKx"}},{"cell_type":"markdown","source":"## Scale dot-product Attention\n\n*The input consists of queries and keys of dimension dk, and values of dimension dv. We compute the dot products of the query with all keys, divide each by the square root of dk, and apply a softmax function to obtain the weights on the values.*\n\n\"Attention is all you need\" paper\n\nThen we use the Q, K and V matrices to calculate the attention scores. **The scores measure how much focus to place on other places or words of the input sequence w.r.t a word at a certain position**. That is, the dot product of the query vector with the key vector of the respective word we're scoring. So, for position 1 we calculate the dot product (.) of q1 and k1, then q1¬†. k2, q1¬†. k3,‚Ä¶¬†\n\nNext we apply the \"scaled\" factor to have more stable gradients. The softmax function can not work properly with large values, resulting in vanishing the gradient and slow down the learning. After \"softmaxing\" we multiply by the Value matrix to keep the values of the words we want to focus on and minimizing or removing the values for the irrelevant words (its value in V matrix should be very small).\n\nThe formula for these operations is:\n\n$Attention(Q, K, V ) = \\text{softmax}\\left(\\dfrac{QK^T}{\\sqrt{d_k}}\\right)V $\n","metadata":{"id":"3sffhwwvX-wj"}},{"cell_type":"code","source":"def scaled_dot_product_attention(queries, keys, values, mask):\n    # Calculate the dot product, QK_transpose\n    product = tf.matmul(queries, keys, transpose_b=True)\n    # Get the scale factor\n    keys_dim = tf.cast(tf.shape(keys)[-1], tf.float32)\n    # Apply the scale factor to the dot product\n    scaled_product = product / tf.math.sqrt(keys_dim)\n    # Apply masking when it is requiered\n    if mask is not None:\n        scaled_product += (mask * -1e9)\n    # dot product with Values\n    attention = tf.matmul(tf.nn.softmax(scaled_product, axis=-1), values)\n    \n    return attention","metadata":{"id":"2rEoCNJURbrT","executionInfo":{"status":"ok","timestamp":1604164522271,"user_tz":-60,"elapsed":1003,"user":{"displayName":"Eduardo Mu√±oz Sala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9gTFB7xfdxQyNiKRUzqE5vwb9WQjQuMtQz5F16w=s64","userId":"13317831924226771761"}},"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T13:39:57.179120Z","iopub.execute_input":"2025-03-03T13:39:57.179447Z","iopub.status.idle":"2025-03-03T13:39:57.184441Z","shell.execute_reply.started":"2025-03-03T13:39:57.179414Z","shell.execute_reply":"2025-03-03T13:39:57.183468Z"}},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":"## Multi-Head Attention\n\nIn the previous description the attention scores are focused on the whole sentence at a time, this would produce the same results even if two sentences contain the same words in a different order. Instead, we would like to attend to different segments of the words. We can give the self attention greater power of discrimination, **by combining several self attention heads**, dividing the words vectors into a fixed number (h, number of heads) of chunks, and then self-attention is applied on the corresponding chunks, using Q, K and V sub-matrices. \n\nThis produce h different output matrices of scores.\n\n![Alt](images/dor_product_multihead.PNG \"title From \"Attention is all you need\" paper by Vaswani, et al., 2017\")\n\nBut the next layer (the Feed-Forward layer) is expecting just one matrix, a vector for each word, so after calculating the dot product of every head, we concat the output matrices and multiply them by an additional weights matrix $W_O$. This final matrix captures information from all the attention heads.\n\n$MultihHead(Q, K, V ) = \\text{Concat}(head_1,...,head_n)W^O$\n\nwhere $head_i=Attention(QW_i^Q,QW_i^K,QW_i^V)$ and $i$ is the head index.\n","metadata":{"id":"-MjtvXrfYEx7"}},{"cell_type":"code","source":"class MultiHeadAttention(layers.Layer):\n    \n    def __init__(self, n_heads):\n        super(MultiHeadAttention, self).__init__()\n        self.n_heads = n_heads\n        \n    def build(self, input_shape):\n        self.d_model = input_shape[-1]\n        assert self.d_model % self.n_heads == 0\n        # Calculate the dimension of every head or projection\n        self.d_head = self.d_model // self.n_heads\n        # Set the weight matrices for Q, K and V\n        self.query_lin = layers.Dense(units=self.d_model)\n        self.key_lin = layers.Dense(units=self.d_model)\n        self.value_lin = layers.Dense(units=self.d_model)\n        # Set the weight matrix for the output of the multi-head attention W0\n        self.final_lin = layers.Dense(units=self.d_model)\n        \n    def split_proj(self, inputs, batch_size): # inputs: (batch_size, seq_length, d_model)\n        # Set the dimension of the projections\n        shape = (batch_size,\n                 -1,\n                 self.n_heads,\n                 self.d_head)\n        # Split the input vectors\n        splited_inputs = tf.reshape(inputs, shape=shape) # (batch_size, seq_length, nb_proj, d_proj)\n        return tf.transpose(splited_inputs, perm=[0, 2, 1, 3]) # (batch_size, nb_proj, seq_length, d_proj)\n    \n    def call(self, queries, keys, values, mask):\n        # Get the batch size\n        batch_size = tf.shape(queries)[0]\n        # Set the Query, Key and Value matrices\n        queries = self.query_lin(queries)\n        keys = self.key_lin(keys)\n        values = self.value_lin(values)\n        # Split Q, K y V between the heads or projections\n        queries = self.split_proj(queries, batch_size)\n        keys = self.split_proj(keys, batch_size)\n        values = self.split_proj(values, batch_size)\n        # Apply the scaled dot product\n        attention = scaled_dot_product_attention(queries, keys, values, mask)\n        # Get the attention scores\n        attention = tf.transpose(attention, perm=[0, 2, 1, 3])\n        # Concat the h heads or projections\n        concat_attention = tf.reshape(attention,\n                                      shape=(batch_size, -1, self.d_model))\n        # Apply W0 to get the output of the multi-head attention\n        outputs = self.final_lin(concat_attention)\n        \n        return outputs","metadata":{"id":"lvq4I9uTX5p7","executionInfo":{"status":"ok","timestamp":1604164525310,"user_tz":-60,"elapsed":995,"user":{"displayName":"Eduardo Mu√±oz Sala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9gTFB7xfdxQyNiKRUzqE5vwb9WQjQuMtQz5F16w=s64","userId":"13317831924226771761"}},"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T13:39:57.185439Z","iopub.execute_input":"2025-03-03T13:39:57.185719Z","iopub.status.idle":"2025-03-03T13:39:57.199250Z","shell.execute_reply.started":"2025-03-03T13:39:57.185691Z","shell.execute_reply":"2025-03-03T13:39:57.198480Z"}},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":"# Positional Encoding\n\nWe mentioned briefly that the order of the words in the sentence is an issue to solve in this model, because the network and the self-attention mechanism is permutation invariant. If we shuffle up the words in the input sentence, we get the same solutions. We need to create a representation of the position of the word in the sentence and add it to the word embedding.\n\n*To this end, we add \"positional encodings\" to the input embeddings at the bottoms of the encoder and decoder stacks. The positional encodings have the same dimension as the embeddings, so that the two can be summed. There are many choices of positional encodings.*\n\n\"Attention is all you need\" paper\n\nSo, we apply a function to map the position in the sentence to real valued vector. The network will learn how to use this information. Another approach would be to use a position embedding, similar to word embedding, coding every known position with a vector. It would requiere sentences of all accepted positions during training but positional encoding allow the model to extrapolate to sequence lengths longer than the ones encountered.\n\nIn the paper a sinusoidal function is applied:","metadata":{"id":"-SBoH8G4XyR9"}},{"cell_type":"markdown","source":"$PE_{(pos,2i)} =\\sin(pos/10000^{2i/dmodel})$\n\n$PE_{(pos,2i+1)} =\\cos(pos/10000^{2i/dmodel})$","metadata":{"id":"7G9C3ucmJ86I"}},{"cell_type":"code","source":"class PositionalEncoding(layers.Layer):\n\n    def __init__(self):\n        super(PositionalEncoding, self).__init__()\n    \n    def get_angles(self, pos, i, d_model): # pos: (seq_length, 1) i: (1, d_model)\n        angles = 1 / np.power(10000., (2*(i//2)) / np.float32(d_model))\n        return pos * angles # (seq_length, d_model)\n\n    def call(self, inputs):\n        # input shape batch_size, seq_length, d_model\n        seq_length = inputs.shape.as_list()[-2]\n        d_model = inputs.shape.as_list()[-1]\n        # Calculate the angles given the input\n        angles = self.get_angles(np.arange(seq_length)[:, np.newaxis],\n                                 np.arange(d_model)[np.newaxis, :],\n                                 d_model)\n        # Calculate the positional encodings\n        angles[:, 0::2] = np.sin(angles[:, 0::2])\n        angles[:, 1::2] = np.cos(angles[:, 1::2])\n        # Expand the encodings with a new dimension\n        pos_encoding = angles[np.newaxis, ...]\n        \n        return inputs + tf.cast(pos_encoding, tf.float32)","metadata":{"id":"e2wc6sYlX0dr","executionInfo":{"status":"ok","timestamp":1604164529236,"user_tz":-60,"elapsed":968,"user":{"displayName":"Eduardo Mu√±oz Sala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9gTFB7xfdxQyNiKRUzqE5vwb9WQjQuMtQz5F16w=s64","userId":"13317831924226771761"}},"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T13:39:57.199978Z","iopub.execute_input":"2025-03-03T13:39:57.200196Z","iopub.status.idle":"2025-03-03T13:39:57.218392Z","shell.execute_reply.started":"2025-03-03T13:39:57.200173Z","shell.execute_reply":"2025-03-03T13:39:57.217787Z"}},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":"# The Encoder\n\nNow that all the main pieces of the model have been described we can introduce the encoder components.¬†\n\n- Positional encoding: Add the position encoding to the input embedding (our input words are transformed to embedding vectors). *\"The same weight matrix is shared between the two embedding layers (encoder and decoder) and the pre-softmax linear transformation. In the embedding layers, we multiply those weights by square root of the model dimension\"* [1], ${\\sqrt{d_{model}}}$.\n\n- N = 6, identical layers, containing two sub-layers: a **multi-head self-attention mechanism**, and a **fully connected feed-forward network**. This FC layer is applied to each position separately and identically and consists of two linear transformations with a ReLU activation in between. But it is applied position-wise to the input, which means that the same neural network is applied to every single \"token\" vector belonging to the sentence sequence.\n\n$$FFN(x)= max(0,xW_1+b_1)W_2+b_2$$\n\n- There is a residual connection around each sub-layer (attention and FC network) followed by a layer normalization.\n\n*Normalization and residual connections are standard tricks used to help deep neural networks train faster and more accurately. The layer normalization is applied over the embedding dimension only.*\n\n**Peter Bloem, \"Transformers from¬†scratch\"**\n\nThe next figure will show the components detailed:\n\n![Alt](images/encoder.PNG \"title \"The Ilustrated Transformer\" by Jay Alammar\")\n\nKeep in mind that **only the vector from the last layer (6-th) is sent to the decoder**.","metadata":{"id":"yiyuHe1OeT5N"}},{"cell_type":"code","source":"class EncoderLayer(layers.Layer):\n    \n    def __init__(self, FFN_units, n_heads, dropout_rate):\n        super(EncoderLayer, self).__init__()\n        # Hidden units of the feed forward component\n        self.FFN_units = FFN_units\n        # Set the number of projectios or heads\n        self.n_heads = n_heads\n        # Dropout rate\n        self.dropout_rate = dropout_rate\n    \n    def build(self, input_shape):\n        self.d_model = input_shape[-1]\n        # Build the multihead layer\n        self.multi_head_attention = MultiHeadAttention(self.n_heads)\n        self.dropout_1 = layers.Dropout(rate=self.dropout_rate)\n        # Layer Normalization\n        self.norm_1 = layers.LayerNormalization(epsilon=1e-6)\n        # Fully connected feed forward layer\n        self.ffn1_relu = layers.Dense(units=self.FFN_units, activation=\"relu\")\n        self.ffn2 = layers.Dense(units=self.d_model)\n        self.dropout_2 = layers.Dropout(rate=self.dropout_rate)\n        # Layer normalization\n        self.norm_2 = layers.LayerNormalization(epsilon=1e-6)\n        \n    def call(self, inputs, mask, training):\n        # Forward pass of the multi-head attention\n        attention = self.multi_head_attention(inputs,\n                                              inputs,\n                                              inputs,\n                                              mask)\n        attention = self.dropout_1(attention, training=training)\n        # Call to the residual connection and layer normalization\n        attention = self.norm_1(attention + inputs)\n        # Call to the FC layer\n        outputs = self.ffn1_relu(attention)\n        outputs = self.ffn2(outputs)\n        outputs = self.dropout_2(outputs, training=training)\n        # Call to residual connection and the layer normalization\n        outputs = self.norm_2(outputs + attention)\n        \n        return outputs","metadata":{"id":"UV0ZMH7KT_KZ","executionInfo":{"status":"ok","timestamp":1604164532307,"user_tz":-60,"elapsed":1003,"user":{"displayName":"Eduardo Mu√±oz Sala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9gTFB7xfdxQyNiKRUzqE5vwb9WQjQuMtQz5F16w=s64","userId":"13317831924226771761"}},"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T13:39:57.219250Z","iopub.execute_input":"2025-03-03T13:39:57.219538Z","iopub.status.idle":"2025-03-03T13:39:57.238858Z","shell.execute_reply.started":"2025-03-03T13:39:57.219509Z","shell.execute_reply":"2025-03-03T13:39:57.238121Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"class Encoder(layers.Layer):\n    \n    def __init__(self,\n                 n_layers,\n                 FFN_units,\n                 n_heads,\n                 dropout_rate,\n                 vocab_size,\n                 d_model,\n                 name=\"encoder\"):\n        super(Encoder, self).__init__(name=name)\n        self.n_layers = n_layers\n        self.d_model = d_model\n        # The embedding layer\n        self.embedding = layers.Embedding(vocab_size, d_model)\n        # Positional encoding layer\n        self.pos_encoding = PositionalEncoding()\n        self.dropout = layers.Dropout(rate=dropout_rate)\n        # Stack of n layers of multi-head attention and FC\n        self.enc_layers = [EncoderLayer(FFN_units,\n                                        n_heads,\n                                        dropout_rate) \n                           for _ in range(n_layers)]\n    \n    def call(self, inputs, mask, training):\n        # Get the embedding vectors\n        outputs = self.embedding(inputs)\n        # Scale the embeddings by sqrt of d_model\n        outputs *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n        # Positional encodding\n        outputs = self.pos_encoding(outputs)\n        outputs = self.dropout(outputs, training=training)\n        # Call the stacked layers\n        for i in range(self.n_layers):\n            outputs = self.enc_layers[i](outputs, mask, training=training)\n\n        return outputs","metadata":{"id":"P-P92KeZih60","executionInfo":{"status":"ok","timestamp":1604164533414,"user_tz":-60,"elapsed":428,"user":{"displayName":"Eduardo Mu√±oz Sala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9gTFB7xfdxQyNiKRUzqE5vwb9WQjQuMtQz5F16w=s64","userId":"13317831924226771761"}},"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T13:39:57.239677Z","iopub.execute_input":"2025-03-03T13:39:57.240029Z","iopub.status.idle":"2025-03-03T13:39:57.258852Z","shell.execute_reply.started":"2025-03-03T13:39:57.239970Z","shell.execute_reply":"2025-03-03T13:39:57.258142Z"}},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":"# The Decoder\n\nThe decoder share some components with the encoder but they are used in a different way to take into account the encoder output.\n\n- Positional encoding: Similar that the one in the encoder\n- N=6 identical layers, containing 3 three sublayers. First, the Masked Multi-head attention or **masked causal attention** to prevent positions from attending to subsequent positions, hiding those features that belong to future states of the sequence. \"This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position i can depend only on the known outputs at positions less than i\" [1]. It is implemented setting to ‚àí‚àû the values corresponding to the forbidden states in the softmax layer of the dot-product attention modules. The second component or **\"encoder-decoder attention\"** performs multi-head attention over the output of the decoder, the Key and Value vectors come from the output of the encoder but the queries come from the previous decoder layer. *This allows every position in the decoder to attend over all positions in the input sequence* [1]. And finally the fully-connected network.\n\n- The residual connection and layer normalization around each sub-layer, similar to the encoder.\n\n![Alt](images/decoder.PNG \"title \"The Ilustrated Transformer\" by Jay Alammar\")\n\nAt the end of the N stacked decoders, the **linear layer**, a fully-connected network, transforms the stacked outputs to a much larger vector, the *logits*. The **softmax layer** then turns those scores (logits) into probabilities (all positive, all add up to 1.0). The cell with the highest probability is chosen, and the word associated with it is produced as the output for this time step.\n","metadata":{"id":"7DthraBEwuvl"}},{"cell_type":"code","source":"class DecoderLayer(layers.Layer):\n    \n    def __init__(self, FFN_units, n_heads, dropout_rate):\n        super(DecoderLayer, self).__init__()\n        self.FFN_units = FFN_units\n        self.n_heads = n_heads\n        self.dropout_rate = dropout_rate\n    \n    def build(self, input_shape):\n        self.d_model = input_shape[-1]\n        \n        # Self multi head attention, causal attention\n        self.multi_head_causal_attention = MultiHeadAttention(self.n_heads)\n        self.dropout_1 = layers.Dropout(rate=self.dropout_rate)\n        self.norm_1 = layers.LayerNormalization(epsilon=1e-6)\n        \n        # Multi head attention, encoder-decoder attention \n        self.multi_head_enc_dec_attention = MultiHeadAttention(self.n_heads)\n        self.dropout_2 = layers.Dropout(rate=self.dropout_rate)\n        self.norm_2 = layers.LayerNormalization(epsilon=1e-6)\n        \n        # Feed foward\n        self.ffn1_relu = layers.Dense(units=self.FFN_units,\n                                    activation=\"relu\")\n        self.ffn2 = layers.Dense(units=self.d_model)\n        self.dropout_3 = layers.Dropout(rate=self.dropout_rate)\n        self.norm_3 = layers.LayerNormalization(epsilon=1e-6)\n        \n    def call(self, inputs, enc_outputs, mask_1, mask_2, training):\n        # Call the masked causal attention\n        attention = self.multi_head_causal_attention(inputs,\n                                                inputs,\n                                                inputs,\n                                                mask_1)\n        attention = self.dropout_1(attention, training=training)\n        # Residual connection and layer normalization\n        attention = self.norm_1(attention + inputs)\n        # Call the encoder-decoder attention\n        attention_2 = self.multi_head_enc_dec_attention(attention,\n                                                  enc_outputs,\n                                                  enc_outputs,\n                                                  mask_2)\n        attention_2 = self.dropout_2(attention_2, training=training)\n        # Residual connection and layer normalization\n        attention_2 = self.norm_2(attention_2 + attention)\n        # Call the Feed forward\n        outputs = self.ffn1_relu(attention_2)\n        outputs = self.ffn2(outputs)\n        outputs = self.dropout_3(outputs, training=training)\n        # Residual connection and layer normalization\n        outputs = self.norm_3(outputs + attention_2)\n        \n        return outputs","metadata":{"id":"7ZWZyFBnwy8u","executionInfo":{"status":"ok","timestamp":1604164536304,"user_tz":-60,"elapsed":1018,"user":{"displayName":"Eduardo Mu√±oz Sala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9gTFB7xfdxQyNiKRUzqE5vwb9WQjQuMtQz5F16w=s64","userId":"13317831924226771761"}},"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T13:39:57.259557Z","iopub.execute_input":"2025-03-03T13:39:57.259749Z","iopub.status.idle":"2025-03-03T13:39:57.280380Z","shell.execute_reply.started":"2025-03-03T13:39:57.259733Z","shell.execute_reply":"2025-03-03T13:39:57.279715Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"class Decoder(layers.Layer):\n    \n    def __init__(self,\n                 n_layers,\n                 FFN_units,\n                 n_heads,\n                 dropout_rate,\n                 vocab_size,\n                 d_model,\n                 name=\"decoder\"):\n        super(Decoder, self).__init__(name=name)\n        self.d_model = d_model\n        self.n_layers = n_layers\n        # Embedding layer\n        self.embedding = layers.Embedding(vocab_size, d_model)\n        # Positional encoding layer\n        self.pos_encoding = PositionalEncoding()\n        self.dropout = layers.Dropout(rate=dropout_rate)\n        # Stacked layers of multi-head attention and feed forward\n        self.dec_layers = [DecoderLayer(FFN_units,\n                                        n_heads,\n                                        dropout_rate) \n                           for _ in range(n_layers)]\n    \n    def call(self, inputs, enc_outputs, mask_1, mask_2, training):\n        # Get the embedding vectors\n        outputs = self.embedding(inputs)\n        # Scale by sqrt of d_model\n        outputs *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n        # Positional encodding\n        outputs = self.pos_encoding(outputs)\n        outputs = self.dropout(outputs, training=training)\n        # Call the stacked layers\n        for i in range(self.n_layers):\n            outputs = self.dec_layers[i](outputs,\n                                         enc_outputs,\n                                         mask_1,\n                                         mask_2,\n                                         training=training)\n\n        return outputs","metadata":{"id":"kpzdiWHiwywF","executionInfo":{"status":"ok","timestamp":1604164538109,"user_tz":-60,"elapsed":907,"user":{"displayName":"Eduardo Mu√±oz Sala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9gTFB7xfdxQyNiKRUzqE5vwb9WQjQuMtQz5F16w=s64","userId":"13317831924226771761"}},"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T13:39:57.281156Z","iopub.execute_input":"2025-03-03T13:39:57.281469Z","iopub.status.idle":"2025-03-03T13:39:57.301681Z","shell.execute_reply.started":"2025-03-03T13:39:57.281436Z","shell.execute_reply":"2025-03-03T13:39:57.300959Z"}},"outputs":[],"execution_count":24},{"cell_type":"markdown","source":"# Transformer\n\nOnce we have defined our components and created the encoder, the decoder and the linear-softmax final layer, we join the pieces to form our model, the Transformer.\n\n![Alt](images/transformer_architecture.PNG \"title \"Attention is all you need\" paper\")","metadata":{"id":"x5sJYkjbz5DD"}},{"cell_type":"code","source":"class Transformer(tf.keras.Model):\n    \n    def __init__(self,\n                 vocab_size_enc,\n                 vocab_size_dec,\n                 d_model,\n                 n_layers,\n                 FFN_units,\n                 n_heads,\n                 dropout_rate,\n                 name=\"transformer\"):\n        super(Transformer, self).__init__(name=name)\n        # Build the encoder\n        self.encoder = Encoder(n_layers,\n                               FFN_units,\n                               n_heads,\n                               dropout_rate,\n                               vocab_size_enc,\n                               d_model)\n        # Build the decoder\n        self.decoder = Decoder(n_layers,\n                               FFN_units,\n                               n_heads,\n                               dropout_rate,\n                               vocab_size_dec,\n                               d_model)\n        # build the linear transformation and softmax function\n        self.last_linear = layers.Dense(units=vocab_size_dec, name=\"lin_ouput\")\n    \n    def create_padding_mask(self, seq): #seq: (batch_size, seq_length)\n        # Create the mask for padding\n        mask = tf.cast(tf.math.equal(seq, 0), tf.float32)\n        return mask[:, tf.newaxis, tf.newaxis, :]\n\n    def create_look_ahead_mask(self, seq):\n        # Create the mask for the causal attention\n        seq_len = tf.shape(seq)[1]\n        look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n        return look_ahead_mask\n    \n    def call(self, enc_inputs, dec_inputs, training):\n        # Create the padding mask for the encoder\n        enc_mask = self.create_padding_mask(enc_inputs)\n        # Create the mask for the causal attention\n        dec_mask_1 = tf.maximum(\n            self.create_padding_mask(dec_inputs),\n            self.create_look_ahead_mask(dec_inputs)\n        )\n        # Create the mask for the encoder-decoder attention\n        dec_mask_2 = self.create_padding_mask(enc_inputs)\n        # Call the encoder\n        enc_outputs = self.encoder(enc_inputs, enc_mask, training=training)\n        # Call the decoder\n        dec_outputs = self.decoder(dec_inputs,\n                                   enc_outputs,\n                                   dec_mask_1,\n                                   dec_mask_2,\n                                   training=training)\n        # Call the Linear and Softmax functions\n        outputs = self.last_linear(dec_outputs)\n        \n        return outputs","metadata":{"id":"GqvqNjJPwyh-","executionInfo":{"status":"ok","timestamp":1604164541547,"user_tz":-60,"elapsed":931,"user":{"displayName":"Eduardo Mu√±oz Sala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9gTFB7xfdxQyNiKRUzqE5vwb9WQjQuMtQz5F16w=s64","userId":"13317831924226771761"}},"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T13:39:57.302383Z","iopub.execute_input":"2025-03-03T13:39:57.302602Z","iopub.status.idle":"2025-03-03T13:39:57.320682Z","shell.execute_reply.started":"2025-03-03T13:39:57.302584Z","shell.execute_reply":"2025-03-03T13:39:57.320127Z"}},"outputs":[],"execution_count":25},{"cell_type":"markdown","source":"It is worth mentioning that we create 3 masks, each of which will allow us:\n- *Encoder mask*: It is a padding mask to discard the pad tokens from the attention calculation.\n- *Decoder mask 1*: this mask is a union of the padding mask and the look ahead mask which will help the causal attention to discard the tokens \"in the future\". We take the maximum value between the padding mask and the look ahead one.\n- *Decoder mask 2*: it is the padding mask and is applied in the encoder-decoder attention layer.\n\nAs you can see then we call the encoder, the decoder and the final linear-softmax layer to get the predicted output from our Transformer model.","metadata":{"id":"4XdnNmy_uEfE"}},{"cell_type":"markdown","source":"# Training the Transformer model\nNow that we have described in detail the components in the paper we are ready to implement them and train a transformer model on a NMT problem. It is a toy problem for educational purposes.\n\nWe need to create a custom loss function to mask the padding tokens and we define the Adam optimizer described in the paper, with beta1 = 0.9, beta2 = 0.98 and epsilon= 10e-9. And then we create a scheduler to vary the learning rate over the training process according to:\n\n$lrate = d_{model}^{-0.5}*min(step\\_num^{-0.5}, step\\_num*warmup\\_steps^{-1.5})$\n","metadata":{"id":"-c-LRThUPrso"}},{"cell_type":"code","source":"def loss_function(target, pred):\n    mask = tf.math.logical_not(tf.math.equal(target, 0))\n    loss_ = loss_object(target, pred)\n    \n    mask = tf.cast(mask, dtype=loss_.dtype)\n    loss_ *= mask\n    \n    return tf.reduce_mean(loss_)\n\nclass CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n    \n    def __init__(self, d_model, warmup_steps=4000):\n        super(CustomSchedule, self).__init__()\n        \n        self.d_model = tf.cast(d_model, tf.float32)\n        self.warmup_steps = warmup_steps\n    \n    def __call__(self, step):\n        step = tf.cast(step, tf.float32)  # Cast step to float to avoid type issues.\n        arg1 = tf.math.rsqrt(step)\n        arg2 = step * (self.warmup_steps ** -1.5)\n        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n","metadata":{"id":"AM6zndzkAxC0","executionInfo":{"status":"ok","timestamp":1604164546949,"user_tz":-60,"elapsed":935,"user":{"displayName":"Eduardo Mu√±oz Sala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9gTFB7xfdxQyNiKRUzqE5vwb9WQjQuMtQz5F16w=s64","userId":"13317831924226771761"}},"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T13:39:57.321465Z","iopub.execute_input":"2025-03-03T13:39:57.321703Z","iopub.status.idle":"2025-03-03T13:39:57.339073Z","shell.execute_reply.started":"2025-03-03T13:39:57.321677Z","shell.execute_reply":"2025-03-03T13:39:57.338283Z"}},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":"And that's all, we have all the necessary elements to train our model using an usual loop for sequence-to-sequence tasks:\n- For every iteration on the batch generator that produce batch size inputs and outputs\n- Get the input sequence from 0 to length-1 and the actual outputs from 1 to length, the next word expected at every sequence step.\n- Call the transformer to get the predictions\n- Calculate the loss function between the real outputs and the predictions\n- Apply the gradients to update the weights in the model\n- Calculate the mean loss and the accuracy for the batch data\n- Show some results and save the model in every epoch","metadata":{"id":"xGarg37Fo36b"}},{"cell_type":"code","source":"from tqdm import tqdm  # Import tqdm for progress bar\n\ndef main_train(dataset, transformer, n_epochs, print_every=50):\n    ''' Train the transformer model for n_epochs using the data generator dataset'''\n    losses = []\n    accuracies = []\n\n    for epoch in range(n_epochs):\n        print(f\"Starting epoch {epoch+1}\")\n        start = time.time()\n\n        # Reset the loss and accuracy calculations\n        train_loss.reset_state()\n        train_accuracy.reset_state()\n\n        # Create a tqdm progress bar for the batch loop\n        progress_bar = tqdm(enumerate(dataset), total=len(dataset), desc=f\"Epoch {epoch+1}\", position=0, leave=True)\n\n        for batch, (enc_inputs, targets) in progress_bar:\n            # Set the decoder inputs\n            dec_inputs = targets[:, :-1]\n            # Set the target outputs, right-shifted\n            dec_outputs_real = targets[:, 1:]\n\n            with tf.GradientTape() as tape:\n                # Call the transformer and get the predicted output\n                predictions = transformer(enc_inputs, dec_inputs, training=True)\n                # Calculate the loss\n                loss = loss_function(dec_outputs_real, predictions)\n\n            # Update the weights and optimizer\n            gradients = tape.gradient(loss, transformer.trainable_variables)\n            optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n\n            # Save and store the metrics\n            train_loss(loss)\n            train_accuracy(dec_outputs_real, predictions)\n\n            # Update tqdm progress bar description with live loss & accuracy\n            progress_bar.set_postfix(loss=train_loss.result().numpy(), accuracy=train_accuracy.result().numpy())\n\n            # Print every `print_every` batches\n            if batch % print_every == 0:\n                losses.append(train_loss.result().numpy())\n                accuracies.append(train_accuracy.result().numpy())\n\n        # Save checkpoint after each epoch\n        ckpt_save_path = ckpt_manager.save()\n        print(f\"Saving checkpoint for epoch {epoch+1} in {ckpt_save_path}\")\n        print(f\"Time for 1 epoch: {time.time() - start:.2f} secs\\n\")\n\n    return losses, accuracies","metadata":{"id":"lhFK5kUx602K","executionInfo":{"status":"ok","timestamp":1604164550739,"user_tz":-60,"elapsed":1004,"user":{"displayName":"Eduardo Mu√±oz Sala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9gTFB7xfdxQyNiKRUzqE5vwb9WQjQuMtQz5F16w=s64","userId":"13317831924226771761"}},"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T13:39:57.339851Z","iopub.execute_input":"2025-03-03T13:39:57.340058Z","iopub.status.idle":"2025-03-03T13:39:57.355511Z","shell.execute_reply.started":"2025-03-03T13:39:57.340041Z","shell.execute_reply":"2025-03-03T13:39:57.354809Z"}},"outputs":[],"execution_count":27},{"cell_type":"markdown","source":"Setting the hyperparameters and parameters of the model and training process:","metadata":{"id":"hLqH4ksXJHPu"}},{"cell_type":"code","source":"# Set hyperparamters for the model\nD_MODEL = 512 # 512\nN_LAYERS = 4 # 6\nFFN_UNITS = 512 # 2048\nN_HEADS = 8 # 8\nDROPOUT_RATE = 0.1 # 0.1\n","metadata":{"id":"QGmrQfakTKUV","executionInfo":{"status":"ok","timestamp":1604164555141,"user_tz":-60,"elapsed":1309,"user":{"displayName":"Eduardo Mu√±oz Sala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9gTFB7xfdxQyNiKRUzqE5vwb9WQjQuMtQz5F16w=s64","userId":"13317831924226771761"}},"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T13:39:57.356307Z","iopub.execute_input":"2025-03-03T13:39:57.356635Z","iopub.status.idle":"2025-03-03T13:39:57.373867Z","shell.execute_reply.started":"2025-03-03T13:39:57.356586Z","shell.execute_reply":"2025-03-03T13:39:57.373256Z"}},"outputs":[],"execution_count":28},{"cell_type":"markdown","source":"Now we define and create all the elements to train the model and evaluate it.","metadata":{"id":"0hCk9aUDqaMo"}},{"cell_type":"code","source":"# Clean the session\ntf.keras.backend.clear_session()\n# Create the Transformer model\ntransformer = Transformer(vocab_size_enc=num_words_inputs,\n                          vocab_size_dec=num_words_output,\n                          d_model=D_MODEL,\n                          n_layers=N_LAYERS,\n                          FFN_units=FFN_UNITS,\n                          n_heads=N_HEADS,\n                          dropout_rate=DROPOUT_RATE)\n\n# Define a categorical cross entropy loss\nloss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True,\n                                                            reduction=\"none\")\n# Define a metric to store the mean loss of every epoch\ntrain_loss = tf.keras.metrics.Mean(name=\"train_loss\")\n# Define a matric to save the accuracy in every epoch\ntrain_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name=\"train_accuracy\")\n# Create the scheduler for learning rate decay\nleaning_rate = CustomSchedule(D_MODEL)\n# Create the Adam optimizer\noptimizer = tf.keras.optimizers.Adam(leaning_rate,\n                                     beta_1=0.9,\n                                     beta_2=0.98,\n                                     epsilon=1e-9)\n        ","metadata":{"id":"qiOdqQ5qPs8z","executionInfo":{"status":"ok","timestamp":1604164565188,"user_tz":-60,"elapsed":888,"user":{"displayName":"Eduardo Mu√±oz Sala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9gTFB7xfdxQyNiKRUzqE5vwb9WQjQuMtQz5F16w=s64","userId":"13317831924226771761"}},"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T13:39:57.374595Z","iopub.execute_input":"2025-03-03T13:39:57.374783Z","iopub.status.idle":"2025-03-03T13:39:57.650236Z","shell.execute_reply.started":"2025-03-03T13:39:57.374767Z","shell.execute_reply":"2025-03-03T13:39:57.649588Z"}},"outputs":[],"execution_count":29},{"cell_type":"markdown","source":"It is very useful to checkpoint and save our model during training. Training can take a lot of time and we can restore the model for future training or use.","metadata":{"id":"wTgnNJbprCdT"}},{"cell_type":"code","source":"#Create the Checkpoint \nckpt = tf.train.Checkpoint(transformer=transformer,\n                           optimizer=optimizer)\n\nckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n\nif ckpt_manager.latest_checkpoint:\n    ckpt.restore(ckpt_manager.latest_checkpoint)\n    print(\"Last checkpoint restored.\")","metadata":{"id":"Nb_32PIU5Zkh","executionInfo":{"status":"ok","timestamp":1604164597668,"user_tz":-60,"elapsed":1883,"user":{"displayName":"Eduardo Mu√±oz Sala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9gTFB7xfdxQyNiKRUzqE5vwb9WQjQuMtQz5F16w=s64","userId":"13317831924226771761"}},"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T13:39:57.650953Z","iopub.execute_input":"2025-03-03T13:39:57.651163Z","iopub.status.idle":"2025-03-03T13:39:57.660955Z","shell.execute_reply.started":"2025-03-03T13:39:57.651145Z","shell.execute_reply":"2025-03-03T13:39:57.660205Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"# Train the model\nlosses, accuracies = main_train(dataset, transformer, EPOCHS, 100)","metadata":{"id":"ND0NAyshNWHD","executionInfo":{"status":"ok","timestamp":1604169439296,"user_tz":-60,"elapsed":4836773,"user":{"displayName":"Eduardo Mu√±oz Sala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9gTFB7xfdxQyNiKRUzqE5vwb9WQjQuMtQz5F16w=s64","userId":"13317831924226771761"}},"outputId":"14be75e6-f9a6-48d4-e248-0550dc859343","colab":{"base_uri":"https://localhost:8080/"},"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T13:39:57.661816Z","iopub.execute_input":"2025-03-03T13:39:57.662069Z","iopub.status.idle":"2025-03-03T15:08:30.583790Z","shell.execute_reply.started":"2025-03-03T13:39:57.662050Z","shell.execute_reply":"2025-03-03T15:08:30.583000Z"}},"outputs":[{"name":"stdout","text":"Starting epoch 1\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 305/305 [05:57<00:00,  1.17s/it, accuracy=0.092, loss=4.5]   \n","output_type":"stream"},{"name":"stdout","text":"Saving checkpoint for epoch 1 in /content/drive/My Drive/Projects/Transformer_NMT/ckpt/ckpt-1\nTime for 1 epoch: 359.25 secs\n\nStarting epoch 2\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 305/305 [05:50<00:00,  1.15s/it, accuracy=0.194, loss=3.01]\n","output_type":"stream"},{"name":"stdout","text":"Saving checkpoint for epoch 2 in /content/drive/My Drive/Projects/Transformer_NMT/ckpt/ckpt-2\nTime for 1 epoch: 351.67 secs\n\nStarting epoch 3\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 305/305 [05:47<00:00,  1.14s/it, accuracy=0.236, loss=2.52]\n","output_type":"stream"},{"name":"stdout","text":"Saving checkpoint for epoch 3 in /content/drive/My Drive/Projects/Transformer_NMT/ckpt/ckpt-3\nTime for 1 epoch: 348.87 secs\n\nStarting epoch 4\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 305/305 [05:47<00:00,  1.14s/it, accuracy=0.263, loss=2.22]\n","output_type":"stream"},{"name":"stdout","text":"Saving checkpoint for epoch 4 in /content/drive/My Drive/Projects/Transformer_NMT/ckpt/ckpt-4\nTime for 1 epoch: 348.45 secs\n\nStarting epoch 5\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 305/305 [05:45<00:00,  1.13s/it, accuracy=0.284, loss=2]   \n","output_type":"stream"},{"name":"stdout","text":"Saving checkpoint for epoch 5 in /content/drive/My Drive/Projects/Transformer_NMT/ckpt/ckpt-5\nTime for 1 epoch: 346.48 secs\n\nStarting epoch 6\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 305/305 [05:44<00:00,  1.13s/it, accuracy=0.307, loss=1.8] \n","output_type":"stream"},{"name":"stdout","text":"Saving checkpoint for epoch 6 in /content/drive/My Drive/Projects/Transformer_NMT/ckpt/ckpt-6\nTime for 1 epoch: 345.75 secs\n\nStarting epoch 7\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 305/305 [05:54<00:00,  1.16s/it, accuracy=0.327, loss=1.63]\n","output_type":"stream"},{"name":"stdout","text":"Saving checkpoint for epoch 7 in /content/drive/My Drive/Projects/Transformer_NMT/ckpt/ckpt-7\nTime for 1 epoch: 355.38 secs\n\nStarting epoch 8\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 305/305 [06:00<00:00,  1.18s/it, accuracy=0.348, loss=1.45]\n","output_type":"stream"},{"name":"stdout","text":"Saving checkpoint for epoch 8 in /content/drive/My Drive/Projects/Transformer_NMT/ckpt/ckpt-8\nTime for 1 epoch: 362.10 secs\n\nStarting epoch 9\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 305/305 [05:55<00:00,  1.16s/it, accuracy=0.368, loss=1.3] \n","output_type":"stream"},{"name":"stdout","text":"Saving checkpoint for epoch 9 in /content/drive/My Drive/Projects/Transformer_NMT/ckpt/ckpt-9\nTime for 1 epoch: 356.54 secs\n\nStarting epoch 10\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 305/305 [05:55<00:00,  1.17s/it, accuracy=0.384, loss=1.17]\n","output_type":"stream"},{"name":"stdout","text":"Saving checkpoint for epoch 10 in /content/drive/My Drive/Projects/Transformer_NMT/ckpt/ckpt-10\nTime for 1 epoch: 356.92 secs\n\nStarting epoch 11\n","output_type":"stream"},{"name":"stderr","text":"Epoch 11: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 305/305 [05:54<00:00,  1.16s/it, accuracy=0.4, loss=1.05]   \n","output_type":"stream"},{"name":"stdout","text":"Saving checkpoint for epoch 11 in /content/drive/My Drive/Projects/Transformer_NMT/ckpt/ckpt-11\nTime for 1 epoch: 355.57 secs\n\nStarting epoch 12\n","output_type":"stream"},{"name":"stderr","text":"Epoch 12: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 305/305 [05:54<00:00,  1.16s/it, accuracy=0.412, loss=0.955]\n","output_type":"stream"},{"name":"stdout","text":"Saving checkpoint for epoch 12 in /content/drive/My Drive/Projects/Transformer_NMT/ckpt/ckpt-12\nTime for 1 epoch: 355.40 secs\n\nStarting epoch 13\n","output_type":"stream"},{"name":"stderr","text":"Epoch 13: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 305/305 [05:54<00:00,  1.16s/it, accuracy=0.422, loss=0.879]\n","output_type":"stream"},{"name":"stdout","text":"Saving checkpoint for epoch 13 in /content/drive/My Drive/Projects/Transformer_NMT/ckpt/ckpt-13\nTime for 1 epoch: 356.27 secs\n\nStarting epoch 14\n","output_type":"stream"},{"name":"stderr","text":"Epoch 14: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 305/305 [05:54<00:00,  1.16s/it, accuracy=0.433, loss=0.8]  \n","output_type":"stream"},{"name":"stdout","text":"Saving checkpoint for epoch 14 in /content/drive/My Drive/Projects/Transformer_NMT/ckpt/ckpt-14\nTime for 1 epoch: 355.51 secs\n\nStarting epoch 15\n","output_type":"stream"},{"name":"stderr","text":"Epoch 15: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 305/305 [05:57<00:00,  1.17s/it, accuracy=0.45, loss=0.693] \n","output_type":"stream"},{"name":"stdout","text":"Saving checkpoint for epoch 15 in /content/drive/My Drive/Projects/Transformer_NMT/ckpt/ckpt-15\nTime for 1 epoch: 358.77 secs\n\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"# Save tokenizers using their native method\ntokenizer_inputs.save_to_file('tokenizer_inputs.subword')\ntokenizer_outputs.save_to_file('tokenizer_outputs.subword')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T15:08:48.904740Z","iopub.execute_input":"2025-03-03T15:08:48.905066Z","iopub.status.idle":"2025-03-03T15:08:48.918781Z","shell.execute_reply.started":"2025-03-03T15:08:48.905039Z","shell.execute_reply":"2025-03-03T15:08:48.917667Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"transformer.save_weights(\"arabic_to_english_transformer_weights.weights.h5\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T15:08:50.887631Z","iopub.execute_input":"2025-03-03T15:08:50.887979Z","iopub.status.idle":"2025-03-03T15:08:51.242940Z","shell.execute_reply.started":"2025-03-03T15:08:50.887953Z","shell.execute_reply":"2025-03-03T15:08:51.242272Z"}},"outputs":[],"execution_count":33},{"cell_type":"markdown","source":"## Show some results from training","metadata":{"id":"nmzyRwDrRGdq"}},{"cell_type":"code","source":"fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15,5))\n# plot some data\nax1.plot(losses, label='loss')\n#plt.plot(results.history['val_loss'], label='val_loss')\nax1.set_title('Training Loss')\nax1.legend()\n# accuracies\nax2.plot(accuracies, label='acc')\n#plt.plot(results.history['val_accuracy_fn'], label='val_acc')\nax2.set_title('Training Accuracy')\nax2.legend()\nplt.show()","metadata":{"id":"hGB3Lv7hthlL","executionInfo":{"status":"ok","timestamp":1604169624585,"user_tz":-60,"elapsed":1126,"user":{"displayName":"Eduardo Mu√±oz Sala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9gTFB7xfdxQyNiKRUzqE5vwb9WQjQuMtQz5F16w=s64","userId":"13317831924226771761"}},"outputId":"6259aaab-db55-4455-ba13-5123b33ffff4","colab":{"base_uri":"https://localhost:8080/","height":336},"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T15:08:58.534163Z","iopub.execute_input":"2025-03-03T15:08:58.534480Z","iopub.status.idle":"2025-03-03T15:08:59.032861Z","shell.execute_reply.started":"2025-03-03T15:08:58.534456Z","shell.execute_reply":"2025-03-03T15:08:59.031903Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 1500x500 with 2 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":34},{"cell_type":"markdown","source":"# Make predictions\n\nWe have seen the scores obtained after training but what we are interested in making predictions and see how the model works with new sentences. The predict function will input a tokenize sentence to the model and return the predicted new sentence, in our example, a translation from english to spanish. \n- Tokenize the input sentence to a sequence of tokens\n- Set the initial output sequence to the SOS token\n- Until we reach the max length or the eos token is returned by the model\n- Get the next word predicted. The model returns the logits, remember that the softmax function is applied in the loss calculation.\n- Get the index in the vocabulary of the word with the highest probability\n- Concat the next word predicted to the output sequence","metadata":{"id":"wnfIU53_0oQi"}},{"cell_type":"code","source":"def predict(inp_sentence, tokenizer_in, tokenizer_out, target_max_len):\n    # Tokenize the input sequence using the tokenizer_in\n    inp_sentence = sos_token_input + tokenizer_in.encode(inp_sentence) + eos_token_input\n    enc_input = tf.expand_dims(inp_sentence, axis=0)\n\n    # Set the initial output sentence to sos\n    out_sentence = sos_token_output\n    # Reshape the output\n    output = tf.expand_dims(out_sentence, axis=0)\n\n    # For max target len tokens\n    for _ in range(target_max_len):\n        # Call the transformer and get the logits \n        predictions = transformer(enc_input, output, training=False) #(1, seq_length, VOCAB_SIZE_ES)\n        # Extract the logists of the next word\n        prediction = predictions[:, -1:, :]\n        # The highest probability is taken\n        predicted_id = tf.cast(tf.argmax(prediction, axis=-1), tf.int32)\n        # Check if it is the eos token\n        if predicted_id == eos_token_output:\n            return tf.squeeze(output, axis=0)\n        # Concat the predicted word to the output sequence\n        output = tf.concat([output, predicted_id], axis=-1)\n\n    return tf.squeeze(output, axis=0)","metadata":{"id":"a6iJ7tPC-pk-","executionInfo":{"status":"ok","timestamp":1604169631392,"user_tz":-60,"elapsed":911,"user":{"displayName":"Eduardo Mu√±oz Sala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9gTFB7xfdxQyNiKRUzqE5vwb9WQjQuMtQz5F16w=s64","userId":"13317831924226771761"}},"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T15:09:11.638780Z","iopub.execute_input":"2025-03-03T15:09:11.639126Z","iopub.status.idle":"2025-03-03T15:09:11.645082Z","shell.execute_reply.started":"2025-03-03T15:09:11.639100Z","shell.execute_reply":"2025-03-03T15:09:11.644106Z"}},"outputs":[],"execution_count":35},{"cell_type":"markdown","source":"And finally our last function receives a sentence in english, calls the transformer to translate it to spanish and shows the result","metadata":{"id":"ECKNvSxUqoow"}},{"cell_type":"code","source":"def translate(sentence):\n    # Get the predicted sequence for the input sentence\n    output = predict(sentence, tokenizer_inputs, tokenizer_outputs, MAX_LENGTH).numpy()\n    # Transform the sequence of tokens to a sentence\n    predicted_sentence = tokenizer_outputs.decode(\n        [i for i in output if i < sos_token_output]\n    )\n\n    return predicted_sentence","metadata":{"id":"s6VeFKrE6Kdx","executionInfo":{"status":"ok","timestamp":1604169634817,"user_tz":-60,"elapsed":961,"user":{"displayName":"Eduardo Mu√±oz Sala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9gTFB7xfdxQyNiKRUzqE5vwb9WQjQuMtQz5F16w=s64","userId":"13317831924226771761"}},"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T15:09:16.518617Z","iopub.execute_input":"2025-03-03T15:09:16.518993Z","iopub.status.idle":"2025-03-03T15:09:16.523418Z","shell.execute_reply.started":"2025-03-03T15:09:16.518965Z","shell.execute_reply":"2025-03-03T15:09:16.522647Z"}},"outputs":[],"execution_count":36},{"cell_type":"markdown","source":"First, we explore the predictions on sentences of our training dataset:","metadata":{"id":"EDtGeTvXokpb"}},{"cell_type":"code","source":"#Show some translations\nsentence = \"ŸÖÿ±ÿ≠ÿ®Ÿãÿßÿå ŸÉŸäŸÅ ÿ≠ÿßŸÑŸÉÿü\"\nprint(\"Input sentence: {}\".format(sentence))\npredicted_sentence = translate(sentence)\nprint(\"Output sentence: {}\".format(predicted_sentence))\nprint(\"Actual: Hello, how are you?\")","metadata":{"id":"5rSkyBdTcY-5","executionInfo":{"status":"ok","timestamp":1604169639117,"user_tz":-60,"elapsed":1228,"user":{"displayName":"Eduardo Mu√±oz Sala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9gTFB7xfdxQyNiKRUzqE5vwb9WQjQuMtQz5F16w=s64","userId":"13317831924226771761"}},"outputId":"23617a58-09c1-4a91-f86f-2cafeaef870b","colab":{"base_uri":"https://localhost:8080/"},"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T15:12:28.100579Z","iopub.execute_input":"2025-03-03T15:12:28.100948Z","iopub.status.idle":"2025-03-03T15:12:29.114183Z","shell.execute_reply.started":"2025-03-03T15:12:28.100920Z","shell.execute_reply":"2025-03-03T15:12:29.113377Z"}},"outputs":[{"name":"stdout","text":"Input sentence: ŸÖÿ±ÿ≠ÿ®Ÿãÿßÿå ŸÉŸäŸÅ ÿ≠ÿßŸÑŸÉÿü\nOutput sentence: How is your problem?\nActual: Hello, how are you?\n","output_type":"stream"}],"execution_count":42},{"cell_type":"code","source":"#Show some translations\nsentence = \"ÿ£ŸÜÿß ÿ£ÿ≠ÿ® ÿ™ÿπŸÑŸÖ ÿßŸÑŸÑÿ∫ÿßÿ™\"\nprint(\"Input sentence: {}\".format(sentence))\npredicted_sentence = translate(sentence)\nprint(\"Output sentence: {}\".format(predicted_sentence))\nprint(\"Actual: I love learning languages.\")","metadata":{"id":"AFTOO58Ncjiu","executionInfo":{"status":"ok","timestamp":1604169644539,"user_tz":-60,"elapsed":1387,"user":{"displayName":"Eduardo Mu√±oz Sala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9gTFB7xfdxQyNiKRUzqE5vwb9WQjQuMtQz5F16w=s64","userId":"13317831924226771761"}},"outputId":"8e4b4473-0939-4f10-a954-8887b2168f0a","colab":{"base_uri":"https://localhost:8080/"},"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T15:13:08.549588Z","iopub.execute_input":"2025-03-03T15:13:08.549981Z","iopub.status.idle":"2025-03-03T15:13:09.376565Z","shell.execute_reply.started":"2025-03-03T15:13:08.549949Z","shell.execute_reply":"2025-03-03T15:13:09.375781Z"}},"outputs":[{"name":"stdout","text":"Input sentence: ÿ£ŸÜÿß ÿ£ÿ≠ÿ® ÿ™ÿπŸÑŸÖ ÿßŸÑŸÑÿ∫ÿßÿ™\nOutput sentence: I like languages.\nActual: I love learning languages.\n","output_type":"stream"}],"execution_count":43},{"cell_type":"markdown","source":"Next, let's predict some new sentences on diferent topics:","metadata":{"id":"tqIIbaB6ovZo"}},{"cell_type":"code","source":"#Show some translations\nsentence = \"ÿßŸÑÿ∑ŸÇÿ≥ ÿßŸÑŸäŸàŸÖ ÿ¨ŸÖŸäŸÑ ÿ¨ÿØŸãÿß.\"\nprint(\"Input sentence: {}\".format(sentence))\npredicted_sentence = translate(sentence)\nprint(\"Output sentence: {}\".format(predicted_sentence))\nprint(\"Actual: The weather is very nice today.\")","metadata":{"id":"BupFjJlgDvCA","executionInfo":{"status":"ok","timestamp":1604169651613,"user_tz":-60,"elapsed":1765,"user":{"displayName":"Eduardo Mu√±oz Sala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9gTFB7xfdxQyNiKRUzqE5vwb9WQjQuMtQz5F16w=s64","userId":"13317831924226771761"}},"outputId":"a57b7102-4afe-4941-b087-d981de9ba4db","colab":{"base_uri":"https://localhost:8080/"},"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T15:14:05.152624Z","iopub.execute_input":"2025-03-03T15:14:05.152971Z","iopub.status.idle":"2025-03-03T15:14:06.350240Z","shell.execute_reply.started":"2025-03-03T15:14:05.152943Z","shell.execute_reply":"2025-03-03T15:14:06.349451Z"}},"outputs":[{"name":"stdout","text":"Input sentence: ÿßŸÑÿ∑ŸÇÿ≥ ÿßŸÑŸäŸàŸÖ ÿ¨ŸÖŸäŸÑ ÿ¨ÿØŸãÿß.\nOutput sentence: Your plan is very nice.\nActual: The weather is very nice today.\n","output_type":"stream"}],"execution_count":44},{"cell_type":"code","source":"#Show some translations\nsentence = \"ÿ£ŸäŸÜ ÿ™ŸÇÿπ ÿ£ŸÇÿ±ÿ® ŸÖÿ≠ÿ∑ÿ© ŸÇÿ∑ÿßÿ±ÿü\"\nprint(\"Input sentence: {}\".format(sentence))\npredicted_sentence = translate(sentence)\nprint(\"Output sentence: {}\".format(predicted_sentence))\nprint(\"Actual: Where is the nearest train station?\")","metadata":{"id":"ZdoWKbCP7Czs","executionInfo":{"status":"ok","timestamp":1604169658722,"user_tz":-60,"elapsed":1236,"user":{"displayName":"Eduardo Mu√±oz Sala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9gTFB7xfdxQyNiKRUzqE5vwb9WQjQuMtQz5F16w=s64","userId":"13317831924226771761"}},"outputId":"98fc6afc-576a-48a4-c9f9-6f4be871b870","colab":{"base_uri":"https://localhost:8080/"},"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T15:14:24.595464Z","iopub.execute_input":"2025-03-03T15:14:24.595813Z","iopub.status.idle":"2025-03-03T15:14:25.936649Z","shell.execute_reply.started":"2025-03-03T15:14:24.595788Z","shell.execute_reply":"2025-03-03T15:14:25.935854Z"}},"outputs":[{"name":"stdout","text":"Input sentence: ÿ£ŸäŸÜ ÿ™ŸÇÿπ ÿ£ŸÇÿ±ÿ® ŸÖÿ≠ÿ∑ÿ© ŸÇÿ∑ÿßÿ±ÿü\nOutput sentence: Where's the train station?\nActual: Where is the nearest train station?\n","output_type":"stream"}],"execution_count":45},{"cell_type":"code","source":"#Show some translations\nsentence = \"ŸáŸÑ ŸäŸÖŸÉŸÜŸÉ ŸÖÿ≥ÿßÿπÿØÿ™Ÿäÿå ŸÖŸÜ ŸÅÿ∂ŸÑŸÉÿü\"\nprint(\"Input sentence: {}\".format(sentence))\npredicted_sentence = translate(sentence)\nprint(\"Output sentence: {}\".format(predicted_sentence))\nprint(\"Actual: Can you help me, please?\")","metadata":{"id":"UGjBEb5WFMGt","executionInfo":{"status":"ok","timestamp":1604169665649,"user_tz":-60,"elapsed":1412,"user":{"displayName":"Eduardo Mu√±oz Sala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9gTFB7xfdxQyNiKRUzqE5vwb9WQjQuMtQz5F16w=s64","userId":"13317831924226771761"}},"outputId":"bdf8d1f1-f827-4ef4-e69b-e6bfc0ed5538","colab":{"base_uri":"https://localhost:8080/"},"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T15:14:46.087953Z","iopub.execute_input":"2025-03-03T15:14:46.088267Z","iopub.status.idle":"2025-03-03T15:14:47.111320Z","shell.execute_reply.started":"2025-03-03T15:14:46.088244Z","shell.execute_reply":"2025-03-03T15:14:47.110552Z"}},"outputs":[{"name":"stdout","text":"Input sentence: ŸáŸÑ ŸäŸÖŸÉŸÜŸÉ ŸÖÿ≥ÿßÿπÿØÿ™Ÿäÿå ŸÖŸÜ ŸÅÿ∂ŸÑŸÉÿü\nOutput sentence: Can you help me?\nActual: Can you help me, please?\n","output_type":"stream"}],"execution_count":46}]}